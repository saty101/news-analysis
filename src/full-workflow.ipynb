{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":["# Text classification"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","df_train = pd.read_csv(\"../input/learn-ai-bbc/BBC News Train.csv\")\n","df_train['category_id'] = df_train['Category'].factorize()[0]\n","df_train.groupby('Category').category_id.count()\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n","features = tfidf.fit_transform(df_train.Text).toarray()\n","labels = df_train.category_id\n","category_to_id = {'business':0, 'tech':1, 'politics':2, 'sport':3, 'entertainment':4}\n","id_to_category = {0: 'business', 1: 'tech', 2: 'politics', 3: 'sport', 4: 'entertainment'}\n"],"execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","model = RandomForestClassifier()\n","\n","#Split Data \n","X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df_train.index, test_size=0.33, random_state=0)\n","\n","#Train Algorithm\n","model.fit(X_train, y_train)\n","\n","# Make Predictions\n","y_pred_proba = model.predict_proba(X_test)\n","y_pred = model.predict(X_test)\n","category_id_df = df_train[['Category', 'category_id']].drop_duplicates().sort_values('category_id')"],"execution_count":117,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["model2 = RandomForestClassifier()\n","model2.fit(features, labels)"],"execution_count":118,"outputs":[{"output_type":"execute_result","execution_count":118,"data":{"text/plain":"RandomForestClassifier()"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["text = 'worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.'"],"execution_count":119,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# The text cell that has been chosen"]},{"metadata":{"trusted":true},"cell_type":"code","source":["text"],"execution_count":120,"outputs":[{"output_type":"execute_result","execution_count":120,"data":{"text/plain":"'worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["test_features = tfidf.transform([text])\n","prediction = model.predict(test_features)\n","id_to_category = {0: 'business', 1: 'tech', 2: 'politics', 3: 'sport', 4: 'entertainment'}\n","for i in range(len(prediction)):\n","    print(id_to_category[prediction[i]])"],"execution_count":121,"outputs":[{"output_type":"stream","text":"business\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Moving onto sentiment analysis"]},{"metadata":{},"cell_type":"markdown","source":["@Rajarshi check the new conditions that I've added for sentiment analysis scores"]},{"metadata":{"trusted":true},"cell_type":"code","source":["from sklearn import preprocessing"],"execution_count":122,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import math\n","import spacy\n","import nltk\n","from nltk.tokenize.toktok import ToktokTokenizer\n","import re\n","from bs4 import BeautifulSoup\n","import unicodedata\n","nltk.download('all', halt_on_error=False)\n","from nltk.corpus import sentiwordnet as swn\n","nlp = spacy.load('en', parse = False, tag=False, entity=False)\n","tokenizer = ToktokTokenizer()\n","\n","def strip_html_tags(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    stripped_text = soup.get_text()\n","    return stripped_text\n","def remove_accented_chars(text):\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return text\n","\n","def remove_special_characters(text):\n","    text = re.sub(r'[^a-zA-z0-9\\s]', '', text)\n","    return text\n","\n","def lemmatize_text(text):\n","    text = nlp(text)\n","    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n","    return text\n","\n","stopword_list = nltk.corpus.stopwords.words('english')\n","stopword_list.remove('no')\n","stopword_list.remove('not')\n","\n","def remove_stopwords(text, is_lower_case=False):\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        filtered_tokens = [token for token in tokens if token not in stopword_list]\n","    else:\n","        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","    filtered_text = ' '.join(filtered_tokens)    \n","    return filtered_text\n","CONTRACTION_MAP = {\n","\"ain't\": \"is not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he'll've\": \"he will have\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'd'y\": \"how do you\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how is\",\n","\"I'd\": \"I would\",\n","\"I'd've\": \"I would have\",\n","\"I'll\": \"I will\",\n","\"I'll've\": \"I will have\",\n","\"I'm\": \"I am\",\n","\"I've\": \"I have\",\n","\"i'd\": \"i would\",\n","\"i'd've\": \"i would have\",\n","\"i'll\": \"i will\",\n","\"i'll've\": \"i will have\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"it'd've\": \"it would have\",\n","\"it'll\": \"it will\",\n","\"it'll've\": \"it will have\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"mightn't've\": \"might not have\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"mustn't've\": \"must not have\",\n","\"needn't\": \"need not\",\n","\"needn't've\": \"need not have\",\n","\"o'clock\": \"of the clock\",\n","\"oughtn't\": \"ought not\",\n","\"oughtn't've\": \"ought not have\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"shan't've\": \"shall not have\",\n","\"she'd\": \"she would\",\n","\"she'd've\": \"she would have\",\n","\"she'll\": \"she will\",\n","\"she'll've\": \"she will have\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"shouldn't've\": \"should not have\",\n","\"so've\": \"so have\",\n","\"so's\": \"so as\",\n","\"that'd\": \"that would\",\n","\"that'd've\": \"that would have\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there would\",\n","\"there'd've\": \"there would have\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'd've\": \"they would have\",\n","\"they'll\": \"they will\",\n","\"they'll've\": \"they will have\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"to've\": \"to have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\n","\"we'd've\": \"we would have\",\n","\"we'll\": \"we will\",\n","\"we'll've\": \"we will have\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\n","\"what'll've\": \"what will have\",\n","\"what're\": \"what are\",\n","\"what's\": \"what is\",\n","\"what've\": \"what have\",\n","\"when's\": \"when is\",\n","\"when've\": \"when have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where is\",\n","\"where've\": \"where have\",\n","\"who'll\": \"who will\",\n","\"who'll've\": \"who will have\",\n","\"who's\": \"who is\",\n","\"who've\": \"who have\",\n","\"why's\": \"why is\",\n","\"why've\": \"why have\",\n","\"will've\": \"will have\",\n","\"won't\": \"will not\",\n","\"won't've\": \"will not have\",\n","\"would've\": \"would have\",\n","\"wouldn't\": \"would not\",\n","\"wouldn't've\": \"would not have\",\n","\"y'all\": \"you all\",\n","\"y'all'd\": \"you all would\",\n","\"y'all'd've\": \"you all would have\",\n","\"y'all're\": \"you all are\",\n","\"y'all've\": \"you all have\",\n","\"you'd\": \"you would\",\n","\"you'd've\": \"you would have\",\n","\"you'll\": \"you will\",\n","\"you'll've\": \"you will have\",\n","\"you're\": \"you are\",\n","\"you've\": \"you have\"\n","}\n","\n","def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n","    \n","    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n","                                      flags=re.IGNORECASE|re.DOTALL)\n","    def expand_match(contraction):\n","        match = contraction.group(0)\n","        first_char = match[0]\n","        expanded_contraction = contraction_mapping.get(match)\\\n","                                if contraction_mapping.get(match)\\\n","                                else contraction_mapping.get(match.lower())                       \n","        expanded_contraction = first_char+expanded_contraction[1:]\n","        return expanded_contraction\n","        \n","    expanded_text = contractions_pattern.sub(expand_match, text)\n","    expanded_text = re.sub(\"'\", \"\", expanded_text)\n","    return expanded_text\n","def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n","                     accented_char_removal=True, text_lower_case=True, \n","                     text_lemmatization=True, special_char_removal=True, \n","                     stopword_removal=True):\n","    \n","    normalized_corpus = []\n","    for doc in corpus:        \n","        if accented_char_removal:\n","            doc = remove_accented_chars(doc)            \n","        if contraction_expansion:\n","            doc = expand_contractions(doc)           \n","        if text_lower_case:\n","            doc = doc.lower()        \n","        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)          \n","        special_char_pattern = re.compile(r'([{.(-)!}])')\n","        doc = special_char_pattern.sub(\" \\\\1 \", doc)        \n","        if text_lemmatization:\n","            doc = lemmatize_text(doc)          \n","        if special_char_removal:\n","            doc = remove_special_characters(doc)        \n","        doc = re.sub(' +', ' ', doc)        \n","        if stopword_removal:\n","            doc = remove_stopwords(doc, is_lower_case=text_lower_case)            \n","        normalized_corpus.append(doc)        \n","    return normalized_corpus\n","nptext = np.array([text])\n","norm_articles = normalize_corpus(nptext)\n","def analyze_sentiment_sentiwordnet_lexicon(article):\n","    tagged_text = [(token.text, token.tag_) for token in nlp(article)]\n","    pos_score = neg_score = token_count = obj_score = 0\n","    for word, tag in tagged_text:\n","        ss_set = None\n","        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n","            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n","#             print(ss_set)\n","        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n","            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n","        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n","            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n","#             ss_set.pos_score()*= 3\n","        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n","            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n","        if ss_set:\n","            if 'JJ' in tag:\n","                pos_score += ss_set.pos_score()*4\n","                neg_score += ss_set.neg_score()*4\n","#                 token_count +=3\n","            elif 'RB' in tag:\n","                pos_score += ss_set.pos_score()*3\n","                neg_score += ss_set.neg_score()*3\n","#                 token_count +=2\n","            elif 'VB' in tag:\n","                pos_score += ss_set.pos_score()*2\n","                neg_score += ss_set.neg_score()*2\n","            else:\n","                pos_score += ss_set.pos_score()\n","                neg_score +=ss_set.neg_score()\n","                token_count += 1\n","            obj_score += ss_set.obj_score()\n","            token_count +=1\n","    final_score = pos_score - neg_score\n","#     print(str(token_count)+'words which are being considered as tokens')\n","    norm_final_score = round(float(final_score) / token_count, 4)\n","#     final_sentiment = math.exp(final_sentiment)\n","#     final_sentiment = math.log(final_score)\n","#     final_sentiment = 'positive' if norm_final_score >= 0.05 else 'negative'\n","#     norm_final_score = (20 * (norm_final_score)/2) - 10\n","    norm_final_score = math.exp(norm_final_score)\n","    norm_final_score = (20 * (norm_final_score)/2.35) - 10\n","    if norm_final_score>=1:\n","        final_sentiment = 'positive'\n","    elif norm_final_score<=-1:\n","        final_sentiment = 'negative'\n","    else:\n","        final_sentiment = 'neutral'\n","    if norm_final_score>10:\n","        norm_final_score = 10\n","    if norm_final_score<-10:\n","        norm_final_score = -10\n","    return final_sentiment, (norm_final_score), final_score\n","predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(article) for article in norm_articles]\n","print('predicted sentiment score:'+ str(predicted_sentiments[0][1]))\n","print('predicted sentiment:' + str(predicted_sentiments[0][0]))"],"execution_count":127,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading collection 'all'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package comparative_sentences is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package comtrans to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /usr/share/nltk_data...\n[nltk_data]    |   Package dolch is already up-to-date!\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package floresta to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package framenet_v15 is already up-to-date!\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package framenet_v17 is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n[nltk_data]    | Downloading package nps_chat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package paradigms to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package ppattach to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package propbank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package state_union to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package verbnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package verbnet3 is already up-to-date!\n[nltk_data]    | Downloading package webtext to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package tagsets to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n[nltk_data]    |       up-to-date!\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package perluniprops is already up-to-date!\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package porter_test to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wmt15_eval is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n","name":"stdout"},{"output_type":"stream","text":"predicted sentiment score:-2.06711367052338\npredicted sentiment:negative\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Moving onto text summarization"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","from collections import defaultdict, OrderedDict\n","from math import sqrt\n","from operator import itemgetter\n","from spacy.tokens import Doc\n","import graphviz\n","import json\n","import logging\n","import networkx as nx\n","import os\n","import os.path\n","import re\n","import string\n","import sys\n","import time\n","import unicodedata\n","\n","PAT_FORWARD = re.compile(\"\\n\\-+ Forwarded message \\-+\\n\")\n","PAT_REPLIED = re.compile(\"\\nOn.*\\d+.*\\n?wrote\\:\\n+\\>\")\n","PAT_UNSUBSC = re.compile(\"\\n\\-+\\nTo unsubscribe,.*\\nFor additional commands,.*\")\n","\n","\n","def split_grafs (lines):\n","    \"\"\"\n","    segment raw text, given as a list of lines, into paragraphs\n","    \"\"\"\n","    graf = []\n","\n","    for line in lines:\n","        line = line.strip()\n","\n","        if len(line) < 1:\n","            if len(graf) > 0:\n","                yield \"\\n\".join(graf)\n","                graf = []\n","        else:\n","            graf.append(line)\n","\n","    if len(graf) > 0:\n","        yield \"\\n\".join(graf)\n","\n","\n","def filter_quotes (text, is_email=True):\n","    \"\"\"\n","    filter the quoted text out of a message\n","    \"\"\"\n","    global PAT_FORWARD, PAT_REPLIED, PAT_UNSUBSC\n","\n","    if is_email:\n","        text = filter(lambda x: x in string.printable, text)\n","\n","        # strip off quoted text in a forward\n","        m = PAT_FORWARD.split(text, re.M)\n","\n","        if m and len(m) > 1:\n","            text = m[0]\n","\n","        # strip off quoted text in a reply\n","        m = PAT_REPLIED.split(text, re.M)\n","\n","        if m and len(m) > 1:\n","            text = m[0]\n","\n","        # strip off any trailing unsubscription notice\n","        m = PAT_UNSUBSC.split(text, re.M)\n","\n","        if m:\n","            text = m[0]\n","\n","    # replace any remaining quoted text with blank lines\n","    lines = []\n","\n","    for line in text.split(\"\\n\"):\n","        if line.startswith(\">\"):\n","            lines.append(\"\")\n","        else:\n","            lines.append(line)\n","\n","    return list(split_grafs(lines))\n","\n","\n","def maniacal_scrubber (text):\n","    \"\"\"\n","    it scrubs the garble from its stream...\n","    or it gets the debugger again\n","    \"\"\"\n","    x = \" \".join(map(lambda s: s.strip(), text.split(\"\\n\"))).strip()\n","\n","    x = x.replace('“', '\"').replace('”', '\"')\n","    x = x.replace(\"‘\", \"'\").replace(\"’\", \"'\").replace(\"`\", \"'\")\n","    x = x.replace(\"…\", \"...\").replace(\"–\", \"-\")\n","\n","    x = str(unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\"))\n","\n","    # some web content returns \"not string\" ?? ostensibly no longer\n","    # possibl in Py 3.x but crazy \"mixed modes\" of character encodings\n","    # have been found in the wild -- YMMV\n","\n","    try:\n","        assert type(x).__name__ == \"str\"\n","    except AssertionError:\n","        print(\"not a string?\", type(line), line)\n","\n","    return x\n","\n","\n","def default_scrubber (text):\n","    \"\"\"\n","    remove spurious punctuation (for English)\n","    \"\"\"\n","    return text.lower().replace(\"'\", \"\")\n","\n","\n","class CollectedPhrase:\n","    \"\"\"\n","    represents one phrase during the collection process\n","    \"\"\"\n","\n","    def __init__ (self, chunk, scrubber):\n","        self.sq_sum_rank = 0.0\n","        self.non_lemma = 0\n","        \n","        self.chunk = chunk\n","        self.text = scrubber(chunk.text)\n","\n","\n","    def __repr__ (self):\n","        return \"{:.4f} ({},{}) {} {}\".format(\n","            self.rank, self.chunk.start, self.chunk.end, self.text, self.key\n","        )\n","\n","\n","    def range (self):\n","        \"\"\"\n","        generate the index range for the span of tokens in this phrase\n","        \"\"\"\n","        return range(self.chunk.start, self.chunk.end)\n","\n","\n","    def set_key (self, compound_key):\n","        \"\"\"\n","        create a unique key for the the phrase based on its lemma components\n","        \"\"\"\n","        self.key = tuple(sorted(list(compound_key)))\n","\n","\n","    def calc_rank (self):\n","        \"\"\"\n","        since noun chunking is greedy, we normalize the rank values\n","        using a point estimate based on the number of non-lemma\n","        tokens within the phrase\n","        \"\"\"\n","        chunk_len = self.chunk.end - self.chunk.start + 1\n","        non_lemma_discount = chunk_len / (chunk_len + (2.0 * self.non_lemma) + 1.0)\n","\n","        # normalize the contributions of all the kept lemma tokens\n","        # within the phrase using root mean square (RMS)\n","\n","        self.rank = sqrt(self.sq_sum_rank / (chunk_len + self.non_lemma)) * non_lemma_discount\n","\n","\n","class Phrase:\n","    \"\"\"\n","    represents one extracted phrase\n","    \"\"\"\n","\n","    def __init__ (self, text, rank, count, phrase_list):\n","        self.text = text\n","        self.rank = rank\n","        self.count = count\n","        self.chunks = [p.chunk for p in phrase_list]\n","\n","\n","    def __repr__ (self):\n","        return self.text\n","\n","\n","class TextRank:\n","    \"\"\"\n","    Python impl of TextRank by Milhacea, et al., as a spaCy extension,\n","    used to extract the top-ranked phrases from a text document\n","    \"\"\"\n","    _EDGE_WEIGHT = 1.0\n","    _POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n","    _TOKEN_LOOKBACK = 3\n","    \n","\n","    def __init__ (\n","            self,\n","            edge_weight=_EDGE_WEIGHT,\n","            logger=None,\n","            pos_kept=_POS_KEPT,\n","            scrubber=default_scrubber,\n","            token_lookback=_TOKEN_LOOKBACK\n","    ):\n","        self.edge_weight = edge_weight\n","        self.logger = logger\n","        self.pos_kept = pos_kept\n","        self.scrubber = scrubber\n","        self.stopwords = defaultdict(list)\n","        self.token_lookback = token_lookback\n","\n","        self.doc = None\n","        self.reset()\n","\n","\n","    def reset (self):\n","        \"\"\"\n","        initialize the data structures needed for extracting phrases\n","        removing any state\n","        \"\"\"\n","        self.elapsed_time = 0.0\n","        self.lemma_graph = nx.Graph()\n","        self.phrases = defaultdict(list)\n","        self.ranks = {}\n","        self.seen_lemma = OrderedDict()\n","\n","\n","    def load_stopwords (self, path=\"stop.json\"):\n","        \"\"\"\n","        load a list of \"stop words\" that get ignored when constructing\n","        the lemma graph -- NB: be cautious when using this feature\n","        \"\"\"\n","        stop_path = None\n","\n","        # check if the path is fully qualified, or if the file is in\n","        # the current working directory\n","\n","        if os.path.isfile(path):\n","            stop_path = path\n","        else:\n","            cwd = os.getcwd()\n","            stop_path = os.path.join(cwd, path)\n","\n","            if not os.path.isfile(stop_path):\n","                loc = os.path.realpath(os.path.join(cwd, os.path.dirname(__file__)))\n","                stop_path = os.path.join(loc, path)\n","\n","        try:\n","            with open(stop_path, \"r\") as f:\n","                data = json.load(f)\n","\n","                for lemma, pos_list in data.items():\n","                    self.stopwords[lemma] = pos_list\n","        except FileNotFoundError:\n","            pass\n","\n","\n","    def increment_edge (self, node0, node1):\n","        \"\"\"\n","        increment the weight for an edge between the two given nodes,\n","        creating the edge first if needed\n","        \"\"\"\n","        if self.logger:\n","            self.logger.debug(\"link {} {}\".format(node0, node1))\n","    \n","        if self.lemma_graph.has_edge(node0, node1):\n","            self.lemma_graph[node0][node1][\"weight\"] += self.edge_weight\n","        else:\n","            self.lemma_graph.add_edge(node0, node1, weight=self.edge_weight)\n","\n","\n","    def link_sentence (self, sent):\n","        \"\"\"\n","        link nodes and edges into the lemma graph for one parsed sentence\n","        \"\"\"\n","        visited_tokens = []\n","        visited_nodes = []\n","\n","        for i in range(sent.start, sent.end):\n","            token = self.doc[i]\n","\n","            if token.pos_ in self.pos_kept:\n","                # skip any stop words...\n","                lemma = token.lemma_.lower().strip()\n","\n","                if lemma in self.stopwords and token.pos_ in self.stopwords[lemma]:\n","                    continue\n","\n","                # ...otherwise proceed\n","                key = (token.lemma_, token.pos_)\n","\n","                if key not in self.seen_lemma:\n","                    self.seen_lemma[key] = set([token.i])\n","                else:\n","                    self.seen_lemma[key].add(token.i)\n","\n","                node_id = list(self.seen_lemma.keys()).index(key)\n","\n","                if not node_id in self.lemma_graph:\n","                    self.lemma_graph.add_node(node_id)\n","\n","                if self.logger:\n","                    self.logger.debug(\"visit {} {}\".format(\n","                        visited_tokens, visited_nodes\n","                    ))\n","                    self.logger.debug(\"range {}\".format(\n","                        list(range(len(visited_tokens) - 1, -1, -1))\n","                    ))\n","            \n","                for prev_token in range(len(visited_tokens) - 1, -1, -1):\n","                    if self.logger:\n","                        self.logger.debug(\"prev_tok {} {}\".format(\n","                            prev_token, (token.i - visited_tokens[prev_token])\n","                        ))\n","                \n","                    if (token.i - visited_tokens[prev_token]) <= self.token_lookback:\n","                        self.increment_edge(node_id, visited_nodes[prev_token])\n","                    else:\n","                        break\n","\n","                if self.logger:\n","                    self.logger.debug(\" -- {} {} {} {} {} {}\".format(\n","                        token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes\n","                    ))\n","\n","                visited_tokens.append(token.i)\n","                visited_nodes.append(node_id)\n","\n","\n","    def collect_phrases (self, chunk):\n","        \"\"\"\n","        collect instances of phrases from the lemma graph\n","        based on the given chunk\n","        \"\"\"\n","        phrase = CollectedPhrase(chunk, self.scrubber)\n","        compound_key = set([])\n","\n","        for i in phrase.range():\n","            token = self.doc[i]\n","            key = (token.lemma_, token.pos_)\n","        \n","            if key in self.seen_lemma:\n","                node_id = list(self.seen_lemma.keys()).index(key)\n","                rank = self.ranks[node_id]\n","                phrase.sq_sum_rank += rank\n","                compound_key.add(key)\n","        \n","                if self.logger:\n","                    self.logger.debug(\" {} {} {} {}\".format(\n","                        token.lemma_, token.pos_, node_id, rank\n","                    ))\n","            else:\n","                phrase.non_lemma += 1\n","    \n","        phrase.set_key(compound_key)\n","        phrase.calc_rank()\n","\n","        self.phrases[phrase.key].append(phrase)\n","\n","        if self.logger:\n","            self.logger.debug(phrase)\n","\n","\n","    def calc_textrank (self):\n","        \"\"\"\n","        iterate through each sentence in the doc, constructing a lemma graph\n","        then returning the top-ranked phrases\n","        \"\"\"\n","        self.reset()\n","        t0 = time.time()\n","\n","        for sent in self.doc.sents:\n","            self.link_sentence(sent)\n","\n","        if self.logger:\n","            self.logger.debug(self.seen_lemma)\n","\n","        # to run the algorithm, we use PageRank – i.e., approximating\n","        # eigenvalue centrality – to calculate ranks for each of the\n","        # nodes in the lemma graph\n","\n","        self.ranks = nx.pagerank(self.lemma_graph)\n","\n","        # collect the top-ranked phrases based on both the noun chunks\n","        # and the named entities\n","\n","        for chunk in self.doc.noun_chunks:\n","            self.collect_phrases(chunk)\n","\n","        for ent in self.doc.ents:\n","            self.collect_phrases(ent)\n","\n","        # since noun chunks can be expressed in different ways (e.g., may\n","        # have articles or prepositions), we need to find a minimum span\n","        # for each phrase based on combinations of lemmas\n","\n","        min_phrases = {}\n","\n","        for phrase_key, phrase_list in self.phrases.items():\n","            phrase_list.sort(key=lambda p: p.rank, reverse=True)\n","            best_phrase = phrase_list[0]\n","            min_phrases[best_phrase.text] = (best_phrase.rank, len(phrase_list), phrase_key)\n","\n","        # yield results\n","\n","        results = sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)\n","\n","        phrase_list = [\n","            Phrase(p, r, c, self.phrases[k]) for p, (r, c, k) in results\n","        ]\n","\n","        t1 = time.time()\n","        self.elapsed_time = (t1 - t0) * 1000.0\n","\n","        return phrase_list\n","\n","\n","    def write_dot (self, path=\"graph.dot\"):\n","        \"\"\"\n","        output the lemma graph in Dot file format\n","        \"\"\"\n","        keys = list(self.seen_lemma.keys())\n","        dot = graphviz.Digraph()\n","\n","        for node_id in self.lemma_graph.nodes():\n","            text = keys[node_id][0].lower()\n","            rank = self.ranks[node_id]\n","            label = \"{} ({:.4f})\".format(text, rank)\n","            dot.node(str(node_id), label)\n","\n","        for edge in self.lemma_graph.edges():\n","            dot.edge(str(edge[0]), str(edge[1]), constraint=\"false\")\n","\n","        with open(path, \"w\") as f:\n","            f.write(dot.source)\n","\n","\n","    def summary (self, limit_phrases=10, limit_sentences=4):\n","        \"\"\"\n","        run extractive summarization, based on vector distance \n","        per sentence from the top-ranked phrases\n","        \"\"\"\n","        unit_vector = []\n","\n","        # construct a list of sentence boundaries with a phrase set\n","        # for each (initialized to empty)\n","\n","        sent_bounds = [ [s.start, s.end, set([])] for s in self.doc.sents ]\n","\n","        # iterate through the top-ranked phrases, added them to the\n","        # phrase vector for each sentence\n","\n","        phrase_id = 0\n","\n","        for p in self.doc._.phrases:\n","            unit_vector.append(p.rank)\n","\n","            if self.logger:\n","                self.logger.debug(\n","                    \"{} {} {}\".format(phrase_id, p.text, p.rank)\n","                )\n","    \n","            for chunk in p.chunks:\n","                for sent_start, sent_end, sent_vector in sent_bounds:\n","                    if chunk.start >= sent_start and chunk.start <= sent_end:\n","                        sent_vector.add(phrase_id)\n","\n","                        if self.logger:\n","                            self.logger.debug(\n","                                \" {} {} {} {}\".format(sent_start, chunk.start, chunk.end, sent_end)\n","                                )\n","\n","                        break\n","\n","            phrase_id += 1\n","\n","            if phrase_id == limit_phrases:\n","                break\n","\n","        # construct a unit_vector for the top-ranked phrases, up to\n","        # the requested limit\n","\n","        sum_ranks = sum(unit_vector)\n","        unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n","\n","        # iterate through each sentence, calculating its euclidean\n","        # distance from the unit vector\n","\n","        sent_rank = {}\n","        sent_id = 0\n","\n","        for sent_start, sent_end, sent_vector in sent_bounds:\n","            sum_sq = 0.0\n","    \n","            for phrase_id in range(len(unit_vector)):\n","                if phrase_id not in sent_vector:\n","                    sum_sq += unit_vector[phrase_id]**2.0\n","\n","            sent_rank[sent_id] = sqrt(sum_sq)\n","            sent_id += 1\n","\n","        # extract the sentences with the lowest distance\n","\n","        sent_text = {}\n","        sent_id = 0\n","\n","        for sent in self.doc.sents:\n","            sent_text[sent_id] = sent\n","            sent_id += 1\n","\n","        # yield results, up to the limit requested\n","\n","        num_sent = 0\n","\n","        for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n","            yield sent_text[sent_id]\n","            num_sent += 1\n","\n","            if num_sent == limit_sentences:\n","                break\n","\n","\n","    def PipelineComponent (self, doc):\n","        \"\"\"\n","        define a custom pipeline component for spaCy and extend the\n","        Doc class to add TextRank\n","        \"\"\"\n","        self.doc = doc\n","        Doc.set_extension(\"phrases\", force=True, default=[])\n","        Doc.set_extension(\"textrank\", force=True, default=self)\n","        doc._.phrases = self.calc_textrank()\n","\n","        return doc\n","\n","length=3\n","tr = TextRank()\n","nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n","doc = nlp(text)\n","sent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\n","limit_phrases = 4\n","\n","phrase_id = 0\n","unit_vector = []\n","\n","for p in doc._.phrases:\n","#     print(phrase_id, p.text, p.rank)\n","    \n","    unit_vector.append(p.rank)\n","    \n","    for chunk in p.chunks:\n","#         print(\" \", chunk.start, chunk.end)\n","        \n","        for sent_start, sent_end, sent_vector in sent_bounds:\n","            if chunk.start >= sent_start and chunk.start <= sent_end:\n","#                 print(\" \", sent_start, chunk.start, chunk.end, sent_end)\n","                sent_vector.add(phrase_id)\n","                break\n","\n","    phrase_id += 1\n","\n","    if phrase_id == limit_phrases:\n","        break\n","sum_ranks = sum(unit_vector)\n","unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n","\n","from math import sqrt\n","\n","sent_rank = {}\n","sent_id = 0\n","\n","for sent_start, sent_end, sent_vector in sent_bounds:\n","#     print(sent_vector)\n","    sum_sq = 0.0\n","    \n","    for phrase_id in range(len(unit_vector)):\n","#         print(phrase_id, unit_vector[phrase_id])\n","        \n","        if phrase_id not in sent_vector:\n","            sum_sq += unit_vector[phrase_id]**2.0\n","\n","    sent_rank[sent_id] = sqrt(sum_sq)\n","    sent_id += 1\n","from operator import itemgetter\n","\n","sorted(sent_rank.items(), key=itemgetter(1)) \n","limit_sentences = length\n","\n","sent_text = {}\n","sent_id = 0\n","\n","for sent in doc.sents:\n","    sent_text[sent_id] = sent.text\n","    sent_id += 1\n","\n","num_sent = 0\n","\n","for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n","    print(sent_text[sent_id])\n","    num_sent += 1\n","    \n","    if num_sent == limit_sentences:\n","        break"],"execution_count":124,"outputs":[{"output_type":"stream","text":"the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books .\nworldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  \nprosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}